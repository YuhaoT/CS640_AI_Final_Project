{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, training, fixed_image_standardization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 16\n",
    "workers = 0 if os.name == 'nt' else 8\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    2726\n",
      "1.0     318\n",
      "2.0     200\n",
      "3.0     112\n",
      "Name: race, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>race</th>\n",
       "      <th>img_path</th>\n",
       "      <th>absolute_img_path</th>\n",
       "      <th>cropped_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12488.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60147.jpeg</td>\n",
       "      <td>./data/profile pics/60147.jpeg</td>\n",
       "      <td>./data/cropped/60147.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719703.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60148.jpeg</td>\n",
       "      <td>./data/profile pics/60148.jpeg</td>\n",
       "      <td>./data/cropped/60148.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>722153.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>profile pics/60149.jpeg</td>\n",
       "      <td>./data/profile pics/60149.jpeg</td>\n",
       "      <td>./data/cropped/60149.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>811618.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>profile pics/60152.jpeg</td>\n",
       "      <td>./data/profile pics/60152.jpeg</td>\n",
       "      <td>./data/cropped/60152.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>822540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60153.jpeg</td>\n",
       "      <td>./data/profile pics/60153.jpeg</td>\n",
       "      <td>./data/cropped/60153.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  race                 img_path               absolute_img_path  \\\n",
       "0   12488.0   0.0  profile pics/60147.jpeg  ./data/profile pics/60147.jpeg   \n",
       "1  719703.0   0.0  profile pics/60148.jpeg  ./data/profile pics/60148.jpeg   \n",
       "2  722153.0   3.0  profile pics/60149.jpeg  ./data/profile pics/60149.jpeg   \n",
       "5  811618.0   3.0  profile pics/60152.jpeg  ./data/profile pics/60152.jpeg   \n",
       "6  822540.0   0.0  profile pics/60153.jpeg  ./data/profile pics/60153.jpeg   \n",
       "\n",
       "                cropped_path  \n",
       "0  ./data/cropped/60147.jpeg  \n",
       "1  ./data/cropped/60148.jpeg  \n",
       "2  ./data/cropped/60149.jpeg  \n",
       "5  ./data/cropped/60152.jpeg  \n",
       "6  ./data/cropped/60153.jpeg  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('./race_prediction_image/data.pkl')\n",
    "data['race'] = np.where(\n",
    "    data['race'] == 4, 0.0, data['race']\n",
    ")\n",
    "data['race'] = np.where(\n",
    "    data['race'] == 5, np.nan, data['race']\n",
    ")\n",
    "data = data.dropna()\n",
    "print(data['race'].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(input):\n",
    "    if os.path.exists(input):\n",
    "        tmp = Image.open(input)\n",
    "        test = tmp.getbands()\n",
    "        keep = tmp.copy()\n",
    "        return keep\n",
    "    else: return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2385, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>race</th>\n",
       "      <th>img_path</th>\n",
       "      <th>absolute_img_path</th>\n",
       "      <th>cropped_path</th>\n",
       "      <th>face</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>811618.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>profile pics/60152.jpeg</td>\n",
       "      <td>./data/profile pics/60152.jpeg</td>\n",
       "      <td>./data/cropped/60152.jpeg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=160x160 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>865071.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60154.jpeg</td>\n",
       "      <td>./data/profile pics/60154.jpeg</td>\n",
       "      <td>./data/cropped/60154.jpeg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=160x160 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>988211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60155.jpeg</td>\n",
       "      <td>./data/profile pics/60155.jpeg</td>\n",
       "      <td>./data/cropped/60155.jpeg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=160x160 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1025311.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>profile pics/60156.jpeg</td>\n",
       "      <td>./data/profile pics/60156.jpeg</td>\n",
       "      <td>./data/cropped/60156.jpeg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=160x160 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1143891.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>profile pics/60157.jpeg</td>\n",
       "      <td>./data/profile pics/60157.jpeg</td>\n",
       "      <td>./data/cropped/60157.jpeg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=160x160 a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  race                 img_path               absolute_img_path  \\\n",
       "5    811618.0   3.0  profile pics/60152.jpeg  ./data/profile pics/60152.jpeg   \n",
       "7    865071.0   0.0  profile pics/60154.jpeg  ./data/profile pics/60154.jpeg   \n",
       "8    988211.0   0.0  profile pics/60155.jpeg  ./data/profile pics/60155.jpeg   \n",
       "9   1025311.0   0.0  profile pics/60156.jpeg  ./data/profile pics/60156.jpeg   \n",
       "10  1143891.0   3.0  profile pics/60157.jpeg  ./data/profile pics/60157.jpeg   \n",
       "\n",
       "                 cropped_path  \\\n",
       "5   ./data/cropped/60152.jpeg   \n",
       "7   ./data/cropped/60154.jpeg   \n",
       "8   ./data/cropped/60155.jpeg   \n",
       "9   ./data/cropped/60156.jpeg   \n",
       "10  ./data/cropped/60157.jpeg   \n",
       "\n",
       "                                                 face  \n",
       "5   <PIL.Image.Image image mode=RGB size=160x160 a...  \n",
       "7   <PIL.Image.Image image mode=RGB size=160x160 a...  \n",
       "8   <PIL.Image.Image image mode=RGB size=160x160 a...  \n",
       "9   <PIL.Image.Image image mode=RGB size=160x160 a...  \n",
       "10  <PIL.Image.Image image mode=RGB size=160x160 a...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['face'] = data['cropped_path'].apply(load_images)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    224\n",
       "0.0    195\n",
       "2.0    136\n",
       "3.0     74\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = data[['face','race']].reset_index(drop=True)\n",
    "white = tmp[tmp['race'] == 0.0].sample(frac=0.9,random_state=2021)\n",
    "tmp = tmp.drop(white.index)\n",
    "x = tmp['face'].tolist()\n",
    "y = tmp['race'].tolist()\n",
    "tmp['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Race_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = torchvision.transforms.Compose(\n",
    "                            [\n",
    "                            np.float32,\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            fixed_image_standardization])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.x[index]),torch.tensor(self.y[index],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalset = Face_Race_Dataset(data['face'].tolist(),data['race'].tolist())\n",
    "dataset = Face_Race_Dataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = (len(dataset) - train_size)\n",
    "train_set,valid_set = torch.utils.data.random_split(dataset,[train_size,valid_size])\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# valid_size = (len(dataset) - train_size)\n",
    "# train_set,valid_size = torch.utils.data.random_split(dataset,[train_size,valid_size])\n",
    "\n",
    "# valid_size = int(valid_size / 2)\n",
    "# test_size = valid_size\n",
    "# valid_set,test_set = torch.utils.data.random_split(valid_size,[valid_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=32,num_workers=workers,shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set,batch_size=32,num_workers=workers,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set,batch_size=32,num_workers=workers,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inception Resnet V1 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes=4\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define optimizer, scheduler, loss, evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(resnet.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial\n",
      "----------\n",
      "Valid |     2/2    | loss:    1.4625 | fps:  339.2374 | acc:    0.2203   \n",
      "\n",
      "Epoch 1/16\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhouy\\AppData\\Local\\Temp/ipykernel_4008/2642461197.py:15: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return self.transform(self.x[index]),torch.tensor(self.y[index],dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train |    18/18   | loss:    1.5749 | fps:  121.3645 | acc:    0.3729   \n",
      "Valid |     2/2    | loss:  642.6880 | fps:  586.1047 | acc:    0.2218   \n",
      "\n",
      "Epoch 2/16\n",
      "----------\n",
      "Train |    18/18   | loss:    1.3522 | fps:  135.1926 | acc:    0.4280   \n",
      "Valid |     2/2    | loss:    9.3128 | fps:  583.1981 | acc:    0.4118   \n",
      "\n",
      "Epoch 3/16\n",
      "----------\n",
      "Train |    18/18   | loss:    1.1830 | fps:  139.9443 | acc:    0.5054   \n",
      "Valid |     2/2    | loss:    1.8553 | fps:  577.8148 | acc:    0.4123   \n",
      "\n",
      "Epoch 4/16\n",
      "----------\n",
      "Train |    18/18   | loss:    1.0249 | fps:  143.0798 | acc:    0.5657   \n",
      "Valid |     2/2    | loss:    1.3955 | fps:  601.6912 | acc:    0.4451   \n",
      "\n",
      "Epoch 5/16\n",
      "----------\n",
      "Train |    18/18   | loss:    1.0261 | fps:  131.7797 | acc:    0.5854   \n",
      "Valid |     2/2    | loss:    1.7553 | fps:  629.8549 | acc:    0.3805   \n",
      "\n",
      "Epoch 6/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.9419 | fps:  139.5529 | acc:    0.6162   \n",
      "Valid |     2/2    | loss:    1.2386 | fps:  636.3835 | acc:    0.4123   \n",
      "\n",
      "Epoch 7/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.8459 | fps:  140.0553 | acc:    0.6526   \n",
      "Valid |     2/2    | loss:    1.2480 | fps:  612.4865 | acc:    0.5066   \n",
      "\n",
      "Epoch 8/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.7575 | fps:  141.1865 | acc:    0.6850   \n",
      "Valid |     2/2    | loss:    1.2797 | fps:  620.5544 | acc:    0.4758   \n",
      "\n",
      "Epoch 9/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.6762 | fps:  133.9531 | acc:    0.7449   \n",
      "Valid |     2/2    | loss:    1.3450 | fps:  602.2399 | acc:    0.4909   \n",
      "\n",
      "Epoch 10/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.6357 | fps:  138.2476 | acc:    0.7517   \n",
      "Valid |     2/2    | loss:    1.4067 | fps:  619.0669 | acc:    0.4753   \n",
      "\n",
      "Epoch 11/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.5437 | fps:  139.7449 | acc:    0.7868   \n",
      "Valid |     2/2    | loss:    1.4789 | fps:  595.4072 | acc:    0.4768   \n",
      "\n",
      "Epoch 12/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.4889 | fps:  135.0225 | acc:    0.8026   \n",
      "Valid |     2/2    | loss:    1.5140 | fps:  567.5641 | acc:    0.4592   \n",
      "\n",
      "Epoch 13/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.4955 | fps:  130.1442 | acc:    0.8193   \n",
      "Valid |     2/2    | loss:    1.5343 | fps:  576.3076 | acc:    0.4753   \n",
      "\n",
      "Epoch 14/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.4577 | fps:  133.3548 | acc:    0.8398   \n",
      "Valid |     2/2    | loss:    1.5057 | fps:  467.4268 | acc:    0.4612   \n",
      "\n",
      "Epoch 15/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.5188 | fps:  138.3951 | acc:    0.7988   \n",
      "Valid |     2/2    | loss:    1.5440 | fps:  593.9687 | acc:    0.4602   \n",
      "\n",
      "Epoch 16/16\n",
      "----------\n",
      "Train |    18/18   | loss:    0.4472 | fps:  138.6724 | acc:    0.8277   \n",
      "Valid |     2/2    | loss:    1.5515 | fps:  587.3810 | acc:    0.4587   \n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, valid_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, valid_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(),'./race_prediction_image/dataset1_pic_race_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhouy\\AppData\\Local\\Temp/ipykernel_4008/2642461197.py:15: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return self.transform(self.x[index]),torch.tensor(self.y[index],dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "resnet.load_state_dict(torch.load('./race_prediction_image/dataset1_pic_race_model.pt'))\n",
    "dataloader = torch.utils.data.DataLoader(totalset,batch_size=32,num_workers=workers,shuffle=False)\n",
    "resnet.eval()\n",
    "total_pred = []\n",
    "total_y = []\n",
    "for i, (x,y) in enumerate(dataloader):\n",
    "    x = x.to(device)\n",
    "    preds = resnet(x)\n",
    "    _, pred_y = torch.max(preds,1)\n",
    "    pred_y = pred_y.detach().cpu().numpy().tolist()\n",
    "    y = y.detach().cpu().numpy().tolist()\n",
    "    total_pred += pred_y\n",
    "    total_y += y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.56      0.71      1951\n",
      "           1       0.40      0.94      0.56       224\n",
      "           2       0.18      0.82      0.29       136\n",
      "           3       0.31      0.42      0.36        74\n",
      "\n",
      "    accuracy                           0.61      2385\n",
      "   macro avg       0.46      0.68      0.48      2385\n",
      "weighted avg       0.85      0.61      0.66      2385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(total_y, total_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f75978233c496711d8b6fbaac206609b5c6c1b124626415f694fad520d3d3bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cs640': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f26cdc8-8ebd-42da-88cf-cb14d3c163be",
   "metadata": {
    "id": "4f26cdc8-8ebd-42da-88cf-cb14d3c163be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6exlB6bQk5hX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6exlB6bQk5hX",
    "outputId": "3539837b-5e81-4c08-a9e3-40d4afef9ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431c5fe2-64b4-40dc-8bf6-839f8a1ec1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    60674\n",
       "0    36115\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a214c-5d0d-4b1f-9099-8862d3284ee9",
   "metadata": {
    "id": "cc4a214c-5d0d-4b1f-9099-8862d3284ee9"
   },
   "source": [
    "sample 10000 data from each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "959db6ce-3fc6-43d6-be94-82d97b4cb19a",
   "metadata": {
    "id": "959db6ce-3fc6-43d6-be94-82d97b4cb19a"
   },
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'label':[0, 1],\n",
    "                     'nostoextract':[10000, 20000], })\n",
    "\n",
    "def bootstrap(data, freq):\n",
    "    freq = freq.set_index('label')\n",
    "\n",
    "    # This function will be applied on each group of instances of the same\n",
    "    # class in `data`.\n",
    "    def sampleClass(classgroup):\n",
    "        cls = classgroup['label'].iloc[0]\n",
    "        nDesired = freq.nostoextract[cls]\n",
    "        nRows = len(classgroup)\n",
    "\n",
    "        nSamples = min(nRows, nDesired)\n",
    "        return classgroup.sample(nSamples)\n",
    "\n",
    "    samples = data.groupby('label').apply(sampleClass)\n",
    "\n",
    "    # If you want a new index with ascending values\n",
    "    # samples.index = range(len(samples))\n",
    "\n",
    "    # If you want an index which is equal to the row in `data` where the sample\n",
    "    # came from\n",
    "    samples.index = samples.index.get_level_values(1)\n",
    "\n",
    "    # If you don't change it then you'll have a multiindex with level 0\n",
    "    # being the class and level 1 being the row in `data` where\n",
    "    # the sample came from.\n",
    "\n",
    "    return samples\n",
    "\n",
    "train_age_df = bootstrap(age_df,freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a5170b9-fdde-4bab-a556-aef32a6ba9d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a5170b9-fdde-4bab-a556-aef32a6ba9d3",
    "outputId": "66651437-1055-4439-951b-896d310c5ae1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b03167e-21d3-4ebf-9448-598a9f5012c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b03167e-21d3-4ebf-9448-598a9f5012c0",
    "outputId": "076df4f6-7380-42bc-fea1-6b82b4b53712"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_age_df = age_df.drop(index = train_age_df.index, axis = 0)\n",
    "test_freq = pd.DataFrame({'label':[0, 1],\n",
    "                     'nostoextract':[10000, 10000], })\n",
    "test_age_df = bootstrap(age_df,test_freq)\n",
    "len(test_age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25be56ee-be99-4ae3-b09f-5416f029ae40",
   "metadata": {
    "id": "25be56ee-be99-4ae3-b09f-5416f029ae40"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_age_df['text'], train_age_df['label'], test_size=0.3, stratify=train_age_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffff85e-7228-49fd-bafe-6144f463358a",
   "metadata": {
    "id": "3ffff85e-7228-49fd-bafe-6144f463358a"
   },
   "source": [
    "### Age prediction using pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "_2chyI5dlro6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2chyI5dlro6",
    "outputId": "19fe9200-e647-4d6f-bde9-30fd1ee81d36"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip insrall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f423886f-9ed3-4c25-a8f3-fbd22d328e61",
   "metadata": {
    "id": "f423886f-9ed3-4c25-a8f3-fbd22d328e61"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d611025-d3cb-49d3-a8a6-85ba8339ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ef7cf0-60c9-44bf-93ba-845f89305522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95ef7cf0-60c9-44bf-93ba-845f89305522",
    "outputId": "87b9f984-ecd9-4fc6-9235-8e682d30dcac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-cased', return_dict=False)\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c43b5d9-9dbc-44f6-ad22-5e3f733b468c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "9c43b5d9-9dbc-44f6-ad22-5e3f733b468c",
    "outputId": "5562b6c7-c595-41c9-857d-2f4b72920f99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in age_df['text']]\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714b0b76-6a3c-4eb4-ad9c-34d260673569",
   "metadata": {
    "id": "714b0b76-6a3c-4eb4-ad9c-34d260673569"
   },
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(), \n",
    "    max_length = 25,\n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67da9856-1597-43ee-852e-047499e7cd7e",
   "metadata": {
    "id": "67da9856-1597-43ee-852e-047499e7cd7e"
   },
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_val.tolist(), \n",
    "    max_length = 25,\n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274e7b2b-f7fb-4cbd-af83-e4dd2fcfd713",
   "metadata": {
    "id": "274e7b2b-f7fb-4cbd-af83-e4dd2fcfd713"
   },
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_age_df['text'].tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ed9931a-7447-4257-b068-7f019a1f0637",
   "metadata": {
    "id": "4ed9931a-7447-4257-b068-7f019a1f0637"
   },
   "outputs": [],
   "source": [
    "# Formatting Ys turn [0, 1, 2, 0, 1, 1] to [[1,0,0], [0,1,0], [0, 0, 1], ...]\n",
    "def label_y2mat(y_ls):\n",
    "    y_mat = np.zeros((len(y_ls), 3))\n",
    "\n",
    "    for idx, v in enumerate(y_ls):\n",
    "        v = int(v)\n",
    "        y_mat[idx][v] = 1\n",
    "\n",
    "    return y_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2ab1d47-e1b4-4834-94cb-aa7c7362d45a",
   "metadata": {
    "id": "f2ab1d47-e1b4-4834-94cb-aa7c7362d45a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train.tolist())\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(y_val.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_age_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03ba81b2-46b9-4814-b796-5855444c96a6",
   "metadata": {
    "id": "03ba81b2-46b9-4814-b796-5855444c96a6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb754835-3b2d-4f39-95ad-31320390be8a",
   "metadata": {
    "id": "eb754835-3b2d-4f39-95ad-31320390be8a"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert, n_classes: int,):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(768, self.bert.config.hidden_size)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.criterion = nn.NLLLoss(weight=weights) \n",
    "\n",
    "        \n",
    "        \n",
    "    # define the forward pass    \n",
    "    def forward(self, sent_id, mask, labels = None):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "829e2783-022f-4dfd-97cd-6167b44caf28",
   "metadata": {
    "id": "829e2783-022f-4dfd-97cd-6167b44caf28"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert, 2)\n",
    "# push the model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6170f61b-4e63-4c5a-bedf-4a0280084f9b",
   "metadata": {
    "id": "6170f61b-4e63-4c5a-bedf-4a0280084f9b"
   },
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "635ae0fe-b3fa-47ff-bea0-e3fbf696778a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "635ae0fe-b3fa-47ff-bea0-e3fbf696778a",
    "outputId": "1c6ffaaa-fc2b-49f5-d02a-b5aa90596327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.5  0.75]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y = y_train)\n",
    "print(\"Class Weights:\",class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72496880-3af4-4a6d-8318-9653a208dacc",
   "metadata": {
    "id": "72496880-3af4-4a6d-8318-9653a208dacc"
   },
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "# number of training epochs\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1faea478-2eb7-454d-80a7-b112214d36e1",
   "metadata": {
    "id": "1faea478-2eb7-454d-80a7-b112214d36e1"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = model.criterion(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "        \n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2673216-479f-493a-ad75-87c620cd649c",
   "metadata": {
    "id": "a2673216-479f-493a-ad75-87c620cd649c"
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = model.criterion(preds,labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "        \n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ef8cbe3-b233-46a2-b518-77e5b3944521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ef8cbe3-b233-46a2-b518-77e5b3944521",
    "outputId": "7f2d4a48-c4a5-47eb-94fc-3287bd2b1aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 8\n",
      "torch.float32 torch.int64\n",
      "torch.float32 torch.int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2z/jckzt5wd62582y2svpvwpcj80000gn/T/ipykernel_28643/2340855948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2z/jckzt5wd62582y2svpvwpcj80000gn/T/ipykernel_28643/2374249423.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# model predictions are stored on GPU. So, push it to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "nYapvkitc_sk",
   "metadata": {
    "id": "nYapvkitc_sk"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f8d0e9-a8b7-46ef-ae1f-94254cc87611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "00f8d0e9-a8b7-46ef-ae1f-94254cc87611",
    "outputId": "7d3fdb3a-d4f2-4c2f-bf92-d2a1da5e626d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2ad50894c957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get predictions for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-e637a53309c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id, mask, labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# define the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                 )\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         )\n\u001b[1;32m    477\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 860.00 MiB (GPU 0; 15.90 GiB total capacity; 14.54 GiB already allocated; 83.75 MiB free; 14.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "y_test = np.argmax(test_y, axis = 1)\n",
    "\n",
    "print(confusion_matrix(y_test, preds, normalize='all'))\n",
    "precision_recall_fscore_support(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1iXruhPn9Zp-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iXruhPn9Zp-",
    "outputId": "d1bcd2e5-f1d4-4894-fe37-c5df5def9e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11491935 0.10282258 0.78225806]\n",
      " [0.06845238 0.20833333 0.72321429]\n",
      " [0.05890115 0.06417946 0.87691939]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.17511521, 0.18018018, 0.85277648]),\n",
       " array([0.11491935, 0.20833333, 0.87691939]),\n",
       " array([0.13877054, 0.19323671, 0.86467944]),\n",
       " array([ 992,  672, 8336]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds, normalize='true'))\n",
    "precision_recall_fscore_support(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sLrhflTvtGMD",
   "metadata": {
    "id": "sLrhflTvtGMD"
   },
   "source": [
    "Below are previous score with BCELoss with dropout rate 0.1 \n",
    "with training set 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "SP3L9EQUz9nH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP3L9EQUz9nH",
    "outputId": "7a8c9b54-b0e8-4639-80db-177297937e7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "_AjtDSFgp8OM",
   "metadata": {
    "id": "_AjtDSFgp8OM"
   },
   "outputs": [],
   "source": [
    "test_y\n",
    "y_test = np.argmax(test_y, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cFttqCt5qBK5",
   "metadata": {
    "id": "cFttqCt5qBK5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "BTobH_68qsJl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTobH_68qsJl",
    "outputId": "519c13c5-b9b2-4c17-9a57-9418a8db4a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 684,  234,  200],\n",
       "       [ 330,  189,   87],\n",
       "       [4763, 1439, 2074]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "JEI1BIxZqyCU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEI1BIxZqyCU",
    "outputId": "3d04ee29-c846-40e2-af29-24ebd8fa402c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.11840055, 0.10150376, 0.87844134]),\n",
       " array([0.6118068 , 0.31188119, 0.25060416]),\n",
       " array([0.19840464, 0.15316045, 0.38995958]),\n",
       " array([1118,  606, 8276]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dYaRdFbVsCCr",
   "metadata": {
    "id": "dYaRdFbVsCCr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

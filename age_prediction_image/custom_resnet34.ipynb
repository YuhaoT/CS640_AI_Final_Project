{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training our own ResNet34 model\n",
    "\n",
    "Follow this paper https://arxiv.org/pdf/1908.04913v1.pdf, we decide to use the UTK face dataset and ResNet34 to build our own model for thie project. Below is the steps to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imutils in /usr4/cs640g/lanfengl/.local/lib/python3.8/site-packages (0.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.6/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Definition of the identity block of the ResNet34\n",
    "def identity_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Definition of Convolutional Block of the ResNet34\n",
    "def convolutional_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Processing Residue with conv(1,1)\n",
    "    x_skip = tf.keras.layers.Conv2D(filter, (1,1), strides = (2,2))(x_skip)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Definition of the ResNet34\n",
    "def ResNet34(shape = (200, 200, 3), classes = 4):\n",
    "    # Step 1 (Setup Input Layer)\n",
    "    x_input = tf.keras.layers.Input(shape)\n",
    "    x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
    "    # Step 2 (Initial Conv layer along with maxPool)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    # Define size of sub-blocks and initial filter size\n",
    "    block_layers = [3, 4, 6, 3]\n",
    "    filter_size = 64\n",
    "    # Step 3 Add the Resnet Blocks\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            # For sub-block 1 Residual/Convolutional block not needed\n",
    "            for j in range(block_layers[i]):\n",
    "                x = identity_block(x, filter_size)\n",
    "        else:\n",
    "            # One Residual/Convolutional Block followed by Identity blocks\n",
    "            # The filter size will go on increasing by a factor of 2\n",
    "            filter_size = filter_size*2\n",
    "            x = convolutional_block(x, filter_size)\n",
    "            for j in range(block_layers[i] - 1):\n",
    "                x = identity_block(x, filter_size)\n",
    "    # Step 4 End Dense Network\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"ResNet34\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age prediction\n",
    "### Preprocessing\n",
    "    We use the dataset UTKFace to train our database, the data set contains totally 23708 images, we randomly select 2708 of them as validation set, 500 of them as test set. The name of each image already contains the label info of the image: the format of the label is [age]_[gender]_[race]_[date&time].jpg.\n",
    "    In the preprocessing, we do:\n",
    "    1. Read the image and turn them into rgb.\n",
    "    2. Shuffle the train set.\n",
    "    3. Normalize the train set and test set, by dividing each pixil value by 255, where 255 is the max value of each pixel.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10346\n",
      "10346\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "image_original_folder = \"../resource/UTKFace_filtered/\"\n",
    "image_paths = list(paths.list_images(image_original_folder))\n",
    "\n",
    "test_set_size = 100 # There are total 23708 images in UTKFace, so this should not exceed 23708\n",
    "non_training_data = random.sample(range(0, 10446), test_set_size)\n",
    "test_indexes = set(non_training_data)\n",
    "\n",
    "train_set = []\n",
    "train_labels = []\n",
    "test_set = []\n",
    "test_labels = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    if i in test_indexes:\n",
    "        added_set = test_set\n",
    "        added_labels = test_labels\n",
    "    else:\n",
    "        added_set = train_set\n",
    "        added_labels = train_labels\n",
    "    # the format of the label is [age]_[gender]_[race]_[date&time].jpg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 4:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    added_set.append(image)\n",
    "    added_labels.append(label)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(train_labels))\n",
    "print(len(test_set))\n",
    "print(len(test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the train data\n",
    "train_XY = list(zip(train_set, train_labels))\n",
    "random.shuffle(train_XY)\n",
    "\n",
    "train_set, train_labels = zip(*train_XY)\n",
    "train_set = list(train_set)\n",
    "train_labels = list(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the data image\n",
    "train_set = np.array(train_set)\n",
    "train_labels = np.array(train_labels)\n",
    "test_set = np.array(test_set)\n",
    "test_labels = np.array(test_labels)\n",
    "normed_train_set = train_set/255\n",
    "normed_test_set = test_set/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "    Besides training the model, we also use the test set to see the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "324/324 [==============================] - 35s 108ms/step - loss: 0.6358 - accuracy: 0.6468\n",
      "Epoch 2/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.5514 - accuracy: 0.7510\n",
      "Epoch 3/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.5140 - accuracy: 0.7881\n",
      "Epoch 4/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.4879 - accuracy: 0.8187\n",
      "Epoch 5/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.4708 - accuracy: 0.8366\n",
      "Epoch 6/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.4525 - accuracy: 0.8579\n",
      "Epoch 7/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.4378 - accuracy: 0.8780\n",
      "Epoch 8/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.4267 - accuracy: 0.8897\n",
      "Epoch 9/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.4121 - accuracy: 0.9047\n",
      "Epoch 10/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.4009 - accuracy: 0.9191\n",
      "Epoch 11/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3901 - accuracy: 0.9315\n",
      "Epoch 12/32\n",
      "324/324 [==============================] - 34s 103ms/step - loss: 0.3802 - accuracy: 0.9431\n",
      "Epoch 13/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.3726 - accuracy: 0.9489\n",
      "Epoch 14/32\n",
      "324/324 [==============================] - 34s 103ms/step - loss: 0.3654 - accuracy: 0.9575\n",
      "Epoch 15/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3606 - accuracy: 0.9614\n",
      "Epoch 16/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3558 - accuracy: 0.9662\n",
      "Epoch 17/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3505 - accuracy: 0.9704\n",
      "Epoch 18/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3478 - accuracy: 0.9725\n",
      "Epoch 19/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3453 - accuracy: 0.9746\n",
      "Epoch 20/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3439 - accuracy: 0.9756\n",
      "Epoch 21/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3410 - accuracy: 0.9776\n",
      "Epoch 22/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.3395 - accuracy: 0.9785\n",
      "Epoch 23/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.3376 - accuracy: 0.9805\n",
      "Epoch 24/32\n",
      "324/324 [==============================] - 34s 103ms/step - loss: 0.3356 - accuracy: 0.9820\n",
      "Epoch 25/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3344 - accuracy: 0.9828\n",
      "Epoch 26/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3341 - accuracy: 0.9831\n",
      "Epoch 27/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3334 - accuracy: 0.9834\n",
      "Epoch 28/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3323 - accuracy: 0.9844\n",
      "Epoch 29/32\n",
      "324/324 [==============================] - 34s 104ms/step - loss: 0.3321 - accuracy: 0.9841\n",
      "Epoch 30/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3315 - accuracy: 0.9850\n",
      "Epoch 31/32\n",
      "324/324 [==============================] - 34s 103ms/step - loss: 0.3302 - accuracy: 0.9860\n",
      "Epoch 32/32\n",
      "324/324 [==============================] - 33s 103ms/step - loss: 0.3298 - accuracy: 0.9861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b264bf219a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 32\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "model = ResNet34(shape = (200, 200, 3) ,classes=2)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=learning_rate, decay_steps=num_epochs*100, decay_rate=0.9)), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "model.fit(normed_train_set, train_labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: 2021_12_6_11_28_age/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"2021_12_6_11_28_age\")\n",
    "model = tf.keras.models.load_model(\"2021_12_6_11_28_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91        52\n",
      "           1       0.91      0.88      0.89        48\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "metrics.accuracy_score(test_labels, results)\n",
    "print(classification_report(y_true=test_labels,y_pred=results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the twitter user profile\n",
    "    Below we will run the age prediction on the first batch of the twitter user profile image.\n",
    "    The preprocess steps are same as what have been down to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7360090446579989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.14      0.00         7\n",
      "           1       1.00      0.74      0.85      1762\n",
      "\n",
      "    accuracy                           0.74      1769\n",
      "   macro avg       0.50      0.44      0.43      1769\n",
      "weighted avg       0.99      0.74      0.84      1769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile0_folder = \"../resource/cropped_profile/\"\n",
    "image_paths = list(paths.list_images(profile0_folder))\n",
    "\n",
    "test_set0 = []\n",
    "test_labels0 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[race]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 3:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    test_set0.append(image)\n",
    "    test_labels0.append(label)\n",
    "\n",
    "test_set0 = np.array(test_set0)\n",
    "test_labels0 = np.array(test_labels0)\n",
    "normed_test_set0 = test_set0/255\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set0)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels0, results))\n",
    "print(classification_report(y_true=test_labels0,y_pred=results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the second user profile set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5560407569141194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.29      0.34       269\n",
      "           1       0.61      0.73      0.67       418\n",
      "\n",
      "    accuracy                           0.56       687\n",
      "   macro avg       0.51      0.51      0.50       687\n",
      "weighted avg       0.53      0.56      0.54       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile1_folder = \"../resource/cropped_profile1/\"\n",
    "image_paths = list(paths.list_images(profile1_folder))\n",
    "\n",
    "test_set1 = []\n",
    "test_labels1 = []\n",
    "test_id1 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 2:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    id = labels[1]\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    test_set1.append(image)\n",
    "    test_id1.append(id)\n",
    "    test_labels1.append(label)\n",
    "\n",
    "test_set1 = np.array(test_set1)\n",
    "test_labels1 = np.array(test_labels1)\n",
    "test_id1 = np.array(test_id1)\n",
    "normed_test_set1 = test_set1/255\n",
    "\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set1)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels1, results))\n",
    "print(classification_report(y_true=test_labels1,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_id1 = test_id1.reshape(-1, 1)\n",
    "test_labels1 = test_labels1.reshape(-1, 1)\n",
    "output_nparray = np.hstack((test_id1, test_labels1, prediction_prob))\n",
    "output_csv = pd.DataFrame(data=output_nparray, columns=[\"id\", \"label\", \"P0\", \"P1\"])\n",
    "output_csv.to_csv(\"age_precition_dataset2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6288870703764321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.30      0.01        10\n",
      "           1       1.00      0.63      0.77      2434\n",
      "\n",
      "    accuracy                           0.63      2444\n",
      "   macro avg       0.50      0.47      0.39      2444\n",
      "weighted avg       0.99      0.63      0.77      2444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile2_folder = \"../resource/cropped_profile2/\"\n",
    "image_paths = list(paths.list_images(profile2_folder))\n",
    "\n",
    "test_set2 = []\n",
    "test_labels2 = []\n",
    "test_id2 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[race]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 3:\n",
    "        # few images in the dataset have broken labels, ignore them\n",
    "        continue\n",
    "    id = labels[2]\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    test_set2.append(image)\n",
    "    test_labels2.append(label)\n",
    "    test_id2.append(id)\n",
    "\n",
    "test_set2 = np.array(test_set2)\n",
    "test_labels2 = np.array(test_labels2)\n",
    "test_id2 = np.array(test_id2)\n",
    "normed_test_set2 = test_set2/255\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set2)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels2, results))\n",
    "print(classification_report(y_true=test_labels2,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_id2 = test_id2.reshape(-1, 1)\n",
    "test_labels2 = test_labels2.reshape(-1, 1)\n",
    "output_nparray = np.hstack((test_id2, test_labels2, prediction_prob))\n",
    "output_csv = pd.DataFrame(data=output_nparray, columns=[\"id\", \"label\", \"P0\", \"P1\"])\n",
    "output_csv.to_csv(\"age_precition_dataset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race prediction\n",
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23605\n",
      "23605\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "image_original_folder = \"../resource/UTKFace/\"\n",
    "image_paths = list(paths.list_images(image_original_folder))\n",
    "\n",
    "test_set_size = 100 # There are total 23708 images in UTKFace, so this should not exceed 23708\n",
    "non_training_data = random.sample(range(0, 23708), test_set_size)\n",
    "test_indexes = set(non_training_data)\n",
    "\n",
    "train_set = []\n",
    "train_labels = []\n",
    "test_set = []\n",
    "test_labels = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    if i in test_indexes:\n",
    "        added_set = test_set\n",
    "        added_labels = test_labels\n",
    "    else:\n",
    "        added_set = train_set\n",
    "        added_labels = train_labels\n",
    "    # the format of the label is [age]_[gender]_[race]_[date&time].jpg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 4:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    \n",
    "    # In UTKFace, there are 4 races, but in our twitter data, there are 5 races, to reach an agreement, here we unify the \n",
    "    # standard:\n",
    "    # 0---White, so for twitter dataset, we need to change 4 to 0\n",
    "    # 1---Black, there is no change needed\n",
    "    # 2---Asian, for twitter dataset, we need to change 3 to 2\n",
    "    # 3---Other, including: Latino/Hispanic/Indian and so on. So for twitter dataset, we need to change 2, 5 to 3; for UTKFace\n",
    "    # dataset, we need to change 4 to 3.\n",
    "    labels = [int(labels[2])]\n",
    "    if labels[0] == 4:\n",
    "        labels[0] = labels[0] - 1\n",
    "    label = labels[0]\n",
    "    image = cv2.imread(imagePath)\n",
    "    added_set.append(image)\n",
    "    added_labels.append(label)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(train_labels))\n",
    "print(len(test_set))\n",
    "print(len(test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# shuffle the train data\n",
    "train_XY = list(zip(train_set, train_labels))\n",
    "random.shuffle(train_XY)\n",
    "\n",
    "train_set, train_labels = zip(*train_XY)\n",
    "train_set = list(train_set)\n",
    "train_labels = list(train_labels)\n",
    "\n",
    "# normalize the data image\n",
    "train_set = np.array(train_set)\n",
    "train_labels = np.array(train_labels)\n",
    "test_set = np.array(test_set)\n",
    "test_labels = np.array(test_labels)\n",
    "normed_train_set = train_set/255\n",
    "normed_test_set = test_set/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is preprocessing for twitter dataset\n",
    "\n",
    "profile0_folder = \"../resource/cropped_profile/\"\n",
    "image_paths = list(paths.list_images(profile0_folder))\n",
    "\n",
    "test_set0 = []\n",
    "test_labels0 = []\n",
    "test_id0 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[race]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 3:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    id = labels[2]\n",
    "    labels = [int(labels[1])]\n",
    "    if labels[0] == 4:\n",
    "        labels[0] = 0\n",
    "    elif labels[0] == 3:\n",
    "        labels[0] = 2\n",
    "    elif labels[0] == 2 or labels[0] == 5:\n",
    "        labels[0] = 3\n",
    "    label = labels[0]\n",
    "    image = cv2.imread(imagePath)\n",
    "    test_set0.append(image)\n",
    "    test_labels0.append(label)\n",
    "    test_id0.append(id)\n",
    "\n",
    "test_set0 = np.array(test_set0)\n",
    "test_labels0 = np.array(test_labels0)\n",
    "test_id0 = np.array(test_id0)\n",
    "normed_test_set0 = test_set0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile2_folder = \"../resource/cropped_profile2/\"\n",
    "image_paths = list(paths.list_images(profile2_folder))\n",
    "\n",
    "test_set2 = []\n",
    "test_labels2 = []\n",
    "test_id2 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[race]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 3:\n",
    "        # few images in the dataset have broken labels, ignore them\n",
    "        continue\n",
    "    id = labels[2]\n",
    "    labels = [int(labels[1])]\n",
    "    if labels[0] == 4:\n",
    "        labels[0] = 0\n",
    "    elif labels[0] == 3:\n",
    "        labels[0] = 2\n",
    "    elif labels[0] == 2 or labels[0] == 5:\n",
    "        labels[0] = 3\n",
    "    label = labels[0]\n",
    "    image = cv2.imread(imagePath)\n",
    "    test_set2.append(image)\n",
    "    test_labels2.append(label)\n",
    "    test_id2.append(id)\n",
    "\n",
    "test_set2 = np.array(test_set2)\n",
    "test_labels2 = np.array(test_labels2)\n",
    "test_id2 = np.array(test_id2)\n",
    "normed_test_set2 = test_set2/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 1.2527 - accuracy: 0.4772\n",
      "Epoch 2/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 1.1877 - accuracy: 0.5461\n",
      "Epoch 3/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 1.1369 - accuracy: 0.6014\n",
      "Epoch 4/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 1.0985 - accuracy: 0.6424\n",
      "Epoch 5/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 1.0558 - accuracy: 0.6874\n",
      "Epoch 6/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 1.0133 - accuracy: 0.7339\n",
      "Epoch 7/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.9803 - accuracy: 0.7702\n",
      "Epoch 8/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.9585 - accuracy: 0.7920\n",
      "Epoch 9/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.9352 - accuracy: 0.8186\n",
      "Epoch 10/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.9165 - accuracy: 0.8382\n",
      "Epoch 11/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.8988 - accuracy: 0.8577\n",
      "Epoch 12/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.8840 - accuracy: 0.8729\n",
      "Epoch 13/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.8694 - accuracy: 0.8875\n",
      "Epoch 14/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8597 - accuracy: 0.8971\n",
      "Epoch 15/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8502 - accuracy: 0.9057\n",
      "Epoch 16/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8400 - accuracy: 0.9160\n",
      "Epoch 17/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8327 - accuracy: 0.9220\n",
      "Epoch 18/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8256 - accuracy: 0.9280\n",
      "Epoch 19/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8192 - accuracy: 0.9339\n",
      "Epoch 20/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.8152 - accuracy: 0.9379\n",
      "Epoch 21/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.8101 - accuracy: 0.9420\n",
      "Epoch 22/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.8055 - accuracy: 0.9459\n",
      "Epoch 23/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.8027 - accuracy: 0.9475\n",
      "Epoch 24/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.8001 - accuracy: 0.9501\n",
      "Epoch 25/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7968 - accuracy: 0.9528\n",
      "Epoch 26/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7950 - accuracy: 0.9541\n",
      "Epoch 27/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7922 - accuracy: 0.9564\n",
      "Epoch 28/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7902 - accuracy: 0.9584\n",
      "Epoch 29/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7890 - accuracy: 0.9589\n",
      "Epoch 30/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7870 - accuracy: 0.9609\n",
      "Epoch 31/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7853 - accuracy: 0.9621\n",
      "Epoch 32/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7836 - accuracy: 0.9635\n",
      "Epoch 33/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7835 - accuracy: 0.9639\n",
      "Epoch 34/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7817 - accuracy: 0.9656\n",
      "Epoch 35/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7810 - accuracy: 0.9659\n",
      "Epoch 36/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7794 - accuracy: 0.9671\n",
      "Epoch 37/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7793 - accuracy: 0.9670\n",
      "Epoch 38/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7782 - accuracy: 0.9679\n",
      "Epoch 39/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7775 - accuracy: 0.9687\n",
      "Epoch 40/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7767 - accuracy: 0.9692\n",
      "Epoch 41/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7761 - accuracy: 0.9696\n",
      "Epoch 42/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7757 - accuracy: 0.9699\n",
      "Epoch 43/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7756 - accuracy: 0.9701\n",
      "Epoch 44/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7750 - accuracy: 0.9705\n",
      "Epoch 45/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7742 - accuracy: 0.9710\n",
      "Epoch 46/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7735 - accuracy: 0.9718\n",
      "Epoch 47/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7733 - accuracy: 0.9720\n",
      "Epoch 48/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7729 - accuracy: 0.9723\n",
      "Epoch 49/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7727 - accuracy: 0.9725\n",
      "Epoch 50/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7724 - accuracy: 0.9727\n",
      "Epoch 51/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7721 - accuracy: 0.9727\n",
      "Epoch 52/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7720 - accuracy: 0.9730\n",
      "Epoch 53/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7717 - accuracy: 0.9731\n",
      "Epoch 54/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7711 - accuracy: 0.9738\n",
      "Epoch 55/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7712 - accuracy: 0.9738\n",
      "Epoch 56/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7713 - accuracy: 0.9736\n",
      "Epoch 57/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7706 - accuracy: 0.9741\n",
      "Epoch 58/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7705 - accuracy: 0.9745\n",
      "Epoch 59/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7702 - accuracy: 0.9745\n",
      "Epoch 60/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7701 - accuracy: 0.9748\n",
      "Epoch 61/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7696 - accuracy: 0.9749\n",
      "Epoch 62/64\n",
      "738/738 [==============================] - 76s 104ms/step - loss: 0.7695 - accuracy: 0.9753\n",
      "Epoch 63/64\n",
      "738/738 [==============================] - 76s 103ms/step - loss: 0.7693 - accuracy: 0.9753\n",
      "Epoch 64/64\n",
      "738/738 [==============================] - 77s 104ms/step - loss: 0.7692 - accuracy: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b232bd6c070>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "model = ResNet34(shape = (200, 200, 3) ,classes=4)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=learning_rate, decay_steps=num_epochs*1000, decay_rate=0.9)), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "model.fit(normed_train_set, train_labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84        44\n",
      "           1       0.75      0.62      0.68        24\n",
      "           2       0.77      0.71      0.74        14\n",
      "           3       0.69      0.61      0.65        18\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.75      0.71      0.73       100\n",
      "weighted avg       0.76      0.76      0.75       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels, results))\n",
    "print(classification_report(y_true=test_labels,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2021_12_6_15_31_race/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"2021_12_6_15_31_race\")\n",
    "model = tf.keras.models.load_model(\"2021_12_6_15_31_race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for twitter users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.604296212549463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76      1423\n",
      "           1       0.22      0.51      0.31       134\n",
      "           2       0.11      0.17      0.13        53\n",
      "           3       0.10      0.18      0.13       159\n",
      "\n",
      "    accuracy                           0.60      1769\n",
      "   macro avg       0.33      0.38      0.33      1769\n",
      "weighted avg       0.73      0.60      0.65      1769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_prob = model.predict(normed_test_set0)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels0, results))\n",
    "print(classification_report(y_true=test_labels0,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5433715220949263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.60      0.71      1926\n",
      "           1       0.19      0.55      0.28       218\n",
      "           2       0.07      0.18      0.10        74\n",
      "           3       0.15      0.20      0.17       226\n",
      "\n",
      "    accuracy                           0.54      2444\n",
      "   macro avg       0.32      0.38      0.32      2444\n",
      "weighted avg       0.71      0.54      0.60      2444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_prob = model.predict(normed_test_set2)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels2, results))\n",
    "print(classification_report(y_true=test_labels2,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_id2 = test_id2.reshape(-1, 1)\n",
    "test_labels2 = test_labels2.reshape(-1, 1)\n",
    "output_nparray = np.hstack((test_id2, test_labels2, prediction_prob))\n",
    "output_csv = pd.DataFrame(data=output_nparray, columns=[\"id\", \"label\", \"P0\", \"P1\", \"P2\", \"P3\"])\n",
    "output_csv.to_csv(\"race_precition_dataset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

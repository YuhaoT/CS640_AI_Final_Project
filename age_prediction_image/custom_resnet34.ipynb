{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Building and Training our own ResNet34 model\n",
    "\n",
    "Follow this paper https://arxiv.org/pdf/1908.04913v1.pdf, we decide to use the UTK face dataset and ResNet34 to build our own model for thie project. Below is the steps to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imutils in /usr4/cs640g/lanfengl/.local/lib/python3.8/site-packages (0.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.6/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Definition of the identity block of the ResNet34\n",
    "def identity_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Definition of Convolutional Block of the ResNet34\n",
    "def convolutional_block(x, filter):\n",
    "    # copy tensor to variable called x_skip\n",
    "    x_skip = x\n",
    "    # Layer 1\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    # Layer 2\n",
    "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    # Processing Residue with conv(1,1)\n",
    "    x_skip = tf.keras.layers.Conv2D(filter, (1,1), strides = (2,2))(x_skip)\n",
    "    # Add Residue\n",
    "    x = tf.keras.layers.Add()([x, x_skip])     \n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Definition of the ResNet34\n",
    "def ResNet34(shape = (200, 200, 3), classes = 4):\n",
    "    # Step 1 (Setup Input Layer)\n",
    "    x_input = tf.keras.layers.Input(shape)\n",
    "    x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
    "    # Step 2 (Initial Conv layer along with maxPool)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    # Define size of sub-blocks and initial filter size\n",
    "    block_layers = [3, 4, 6, 3]\n",
    "    filter_size = 64\n",
    "    # Step 3 Add the Resnet Blocks\n",
    "    for i in range(4):\n",
    "        if i == 0:\n",
    "            # For sub-block 1 Residual/Convolutional block not needed\n",
    "            for j in range(block_layers[i]):\n",
    "                x = identity_block(x, filter_size)\n",
    "        else:\n",
    "            # One Residual/Convolutional Block followed by Identity blocks\n",
    "            # The filter size will go on increasing by a factor of 2\n",
    "            filter_size = filter_size*2\n",
    "            x = convolutional_block(x, filter_size)\n",
    "            for j in range(block_layers[i] - 1):\n",
    "                x = identity_block(x, filter_size)\n",
    "    # Step 4 End Dense Network\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), padding = 'same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"ResNet34\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "    We use the dataset UTKFace to train our database, the data set contains totally 23708 images, we randomly select 2708 of them as validation set, 500 of them as test set. The name of each image already contains the label info of the image: the format of the label is [age]_[gender]_[race]_[date&time].jpg.\n",
    "    In the preprocessing, we do:\n",
    "    1. Read the image and turn them into rgb.\n",
    "    2. Shuffle the train set.\n",
    "    3. Normalize the train set and test set, by dividing each pixil value by 255, where 255 is the max value of each pixel.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10346\n",
      "10346\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "image_original_folder = \"../resource/UTKFace_filtered/\"\n",
    "image_paths = list(paths.list_images(image_original_folder))\n",
    "\n",
    "test_set_size = 100 # There are total 23708 images in UTKFace, so this should not exceed 23708\n",
    "non_training_data = random.sample(range(0, 10446), test_set_size)\n",
    "test_indexes = set(non_training_data)\n",
    "\n",
    "train_set = []\n",
    "train_labels = []\n",
    "test_set = []\n",
    "test_labels = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    if i in test_indexes:\n",
    "        added_set = test_set\n",
    "        added_labels = test_labels\n",
    "    else:\n",
    "        added_set = train_set\n",
    "        added_labels = train_labels\n",
    "    # the format of the label is [age]_[gender]_[race]_[date&time].jpg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 4:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    added_set.append(gray)\n",
    "    added_labels.append(label)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(train_labels))\n",
    "print(len(test_set))\n",
    "print(len(test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the train data\n",
    "train_XY = list(zip(train_set, train_labels))\n",
    "random.shuffle(train_XY)\n",
    "\n",
    "train_set, train_labels = zip(*train_XY)\n",
    "train_set = list(train_set)\n",
    "train_labels = list(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the data image\n",
    "train_set = np.array(train_set)\n",
    "train_labels = np.array(train_labels)\n",
    "test_set = np.array(test_set)\n",
    "test_labels = np.array(test_labels)\n",
    "normed_train_set = train_set/255\n",
    "normed_test_set = test_set/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "    Besides training the model, we also use the test set to see the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "  2/324 [..............................] - ETA: 16s - loss: 0.8070 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0400s vs `on_train_batch_end` time: 0.0602s). Check your callbacks.\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.6208 - accuracy: 0.6640\n",
      "Epoch 2/32\n",
      "324/324 [==============================] - 32s 100ms/step - loss: 0.5346 - accuracy: 0.7678\n",
      "Epoch 3/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.5067 - accuracy: 0.7969\n",
      "Epoch 4/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4865 - accuracy: 0.8211\n",
      "Epoch 5/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4701 - accuracy: 0.8383\n",
      "Epoch 6/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4606 - accuracy: 0.8456\n",
      "Epoch 7/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4458 - accuracy: 0.8657\n",
      "Epoch 8/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4352 - accuracy: 0.8754\n",
      "Epoch 9/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4269 - accuracy: 0.8859\n",
      "Epoch 10/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4149 - accuracy: 0.9006\n",
      "Epoch 11/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.4038 - accuracy: 0.9139\n",
      "Epoch 12/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3962 - accuracy: 0.9214\n",
      "Epoch 13/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3849 - accuracy: 0.9360\n",
      "Epoch 14/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3772 - accuracy: 0.9435\n",
      "Epoch 15/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3695 - accuracy: 0.9519\n",
      "Epoch 16/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3626 - accuracy: 0.9581\n",
      "Epoch 17/32\n",
      "324/324 [==============================] - 32s 100ms/step - loss: 0.3586 - accuracy: 0.9613\n",
      "Epoch 18/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3539 - accuracy: 0.9668\n",
      "Epoch 19/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3499 - accuracy: 0.9696\n",
      "Epoch 20/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3484 - accuracy: 0.9709\n",
      "Epoch 21/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3448 - accuracy: 0.9740\n",
      "Epoch 22/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3423 - accuracy: 0.9758\n",
      "Epoch 23/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3410 - accuracy: 0.9768\n",
      "Epoch 24/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3396 - accuracy: 0.9782\n",
      "Epoch 25/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3386 - accuracy: 0.9784\n",
      "Epoch 26/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3366 - accuracy: 0.9808\n",
      "Epoch 27/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3361 - accuracy: 0.9809\n",
      "Epoch 28/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3348 - accuracy: 0.9821\n",
      "Epoch 29/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3335 - accuracy: 0.9828\n",
      "Epoch 30/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3331 - accuracy: 0.9829\n",
      "Epoch 31/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3324 - accuracy: 0.9829\n",
      "Epoch 32/32\n",
      "324/324 [==============================] - 32s 99ms/step - loss: 0.3322 - accuracy: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2adb94e1d250>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: should we nnormalize the image vectors?\n",
    "\n",
    "num_epochs = 32\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "model = ResNet34(shape = (200, 200, 1) ,classes=2)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=learning_rate, decay_steps=num_epochs*100, decay_rate=0.9)), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "model.fit(normed_train_set, train_labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2021_12_4_15_46/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"2021_12_4_15_46\")\n",
    "model = tf.keras.models.load_model(\"2021_12_4_15_46\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.60      0.74        48\n",
      "           1       0.73      0.98      0.84        52\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.85      0.79      0.79       100\n",
      "weighted avg       0.84      0.80      0.79       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "metrics.accuracy_score(test_labels, results)\n",
    "print(classification_report(y_true=test_labels,y_pred=results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the twitter user profile\n",
    "    Below we will run the age prediction on the first batch of the twitter user profile image.\n",
    "    The preprocess steps are same as what have been down to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92312040700961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       1.00      0.93      0.96      1762\n",
      "\n",
      "    accuracy                           0.92      1769\n",
      "   macro avg       0.50      0.46      0.48      1769\n",
      "weighted avg       0.99      0.92      0.96      1769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile0_folder = \"../resource/cropped_profile/\"\n",
    "image_paths = list(paths.list_images(profile0_folder))\n",
    "\n",
    "test_set0 = []\n",
    "test_labels0 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[race]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 3:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    test_set0.append(gray)\n",
    "    test_labels0.append(label)\n",
    "\n",
    "test_set0 = np.array(test_set0)\n",
    "test_labels0 = np.array(test_labels0)\n",
    "normed_test_set0 = test_set0/255\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set0)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels0, results))\n",
    "print(classification_report(y_true=test_labels0,y_pred=results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the second user profile set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586608442503639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.09      0.15       269\n",
      "           1       0.61      0.90      0.73       418\n",
      "\n",
      "    accuracy                           0.59       687\n",
      "   macro avg       0.50      0.50      0.44       687\n",
      "weighted avg       0.52      0.59      0.50       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile1_folder = \"../resource/cropped_profile1/\"\n",
    "image_paths = list(paths.list_images(profile1_folder))\n",
    "\n",
    "test_set1 = []\n",
    "test_labels1 = []\n",
    "for (i, imagePath) in enumerate(image_paths):\n",
    "    # the format of the label is [age]_[id].jpeg,\n",
    "    # here only age and race are needed\n",
    "    labels = imagePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "    if len(labels) != 2:\n",
    "        # few images in the UTKFace have broken labels, ignore them\n",
    "        continue\n",
    "    labels = [int(labels[0])]\n",
    "    label = 0 if labels[0] < 21 else 1\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    test_set1.append(gray)\n",
    "    test_labels1.append(label)\n",
    "\n",
    "test_set1 = np.array(test_set1)\n",
    "test_labels1 = np.array(test_labels1)\n",
    "normed_test_set1 = test_set1/255\n",
    "\n",
    "prediction_prob = model.predict(normed_test_set1)\n",
    "results = tf.argmax(prediction_prob, axis=1)\n",
    "print(metrics.accuracy_score(test_labels1, results))\n",
    "print(classification_report(y_true=test_labels1,y_pred=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
